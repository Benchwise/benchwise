{"allContent":{"docusaurus-plugin-css-cascade-layers":{},"docusaurus-plugin-content-docs":{"default":{"loadedVersions":[{"versionName":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","path":"/benchwise/docs","tagsPath":"/benchwise/docs/tags","editUrl":"https://github.com/Benchwise/benchwise/tree/main/website/docs","isLast":true,"routePriority":-1,"sidebarFilePath":"/Users/anurag/Developer/benchwise/website/sidebars.ts","contentPath":"/Users/anurag/Developer/benchwise/website/docs","docs":[{"id":"advanced/api-integration","title":"API Integration","description":"Integrate with the Benchwise platform API.","source":"@site/docs/advanced/api-integration.md","sourceDirName":"advanced","slug":"/advanced/api-integration","permalink":"/benchwise/docs/advanced/api-integration","draft":false,"unlisted":false,"editUrl":"https://github.com/Benchwise/benchwise/tree/main/website/docs/advanced/api-integration.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"docs","previous":{"title":"Configuration","permalink":"/benchwise/docs/advanced/configuration"},"next":{"title":"Custom Metrics","permalink":"/benchwise/docs/advanced/custom-metrics"}},{"id":"advanced/configuration","title":"Configuration","description":"Advanced configuration options for Benchwise.","source":"@site/docs/advanced/configuration.md","sourceDirName":"advanced","slug":"/advanced/configuration","permalink":"/benchwise/docs/advanced/configuration","draft":false,"unlisted":false,"editUrl":"https://github.com/Benchwise/benchwise/tree/main/website/docs/advanced/configuration.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"docs","previous":{"title":"Multi-Model Comparison","permalink":"/benchwise/docs/examples/multi-model-comparison"},"next":{"title":"API Integration","permalink":"/benchwise/docs/advanced/api-integration"}},{"id":"advanced/custom-metrics","title":"Custom Metrics","description":"Create custom evaluation metrics.","source":"@site/docs/advanced/custom-metrics.md","sourceDirName":"advanced","slug":"/advanced/custom-metrics","permalink":"/benchwise/docs/advanced/custom-metrics","draft":false,"unlisted":false,"editUrl":"https://github.com/Benchwise/benchwise/tree/main/website/docs/advanced/custom-metrics.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"docs","previous":{"title":"API Integration","permalink":"/benchwise/docs/advanced/api-integration"},"next":{"title":"Error Handling","permalink":"/benchwise/docs/advanced/error-handling"}},{"id":"advanced/error-handling","title":"Error Handling","description":"Handle errors gracefully in evaluations.","source":"@site/docs/advanced/error-handling.md","sourceDirName":"advanced","slug":"/advanced/error-handling","permalink":"/benchwise/docs/advanced/error-handling","draft":false,"unlisted":false,"editUrl":"https://github.com/Benchwise/benchwise/tree/main/website/docs/advanced/error-handling.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"docs","previous":{"title":"Custom Metrics","permalink":"/benchwise/docs/advanced/custom-metrics"},"next":{"title":"Logging","permalink":"/benchwise/docs/advanced/logging"}},{"id":"advanced/logging","title":"Logging","description":"Configure logging for debugging and monitoring.","source":"@site/docs/advanced/logging.md","sourceDirName":"advanced","slug":"/advanced/logging","permalink":"/benchwise/docs/advanced/logging","draft":false,"unlisted":false,"editUrl":"https://github.com/Benchwise/benchwise/tree/main/website/docs/advanced/logging.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5},"sidebar":"docs","previous":{"title":"Error Handling","permalink":"/benchwise/docs/advanced/error-handling"},"next":{"title":"Offline Mode","permalink":"/benchwise/docs/advanced/offline-mode"}},{"id":"advanced/offline-mode","title":"Offline Mode","description":"Run evaluations without internet connectivity.","source":"@site/docs/advanced/offline-mode.md","sourceDirName":"advanced","slug":"/advanced/offline-mode","permalink":"/benchwise/docs/advanced/offline-mode","draft":false,"unlisted":false,"editUrl":"https://github.com/Benchwise/benchwise/tree/main/website/docs/advanced/offline-mode.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"sidebar_position":6},"sidebar":"docs","previous":{"title":"Logging","permalink":"/benchwise/docs/advanced/logging"},"next":{"title":"CLI","permalink":"/benchwise/docs/cli"}},{"id":"api/client","title":"Benchwise Client","description":"API client for Benchwise platform.","source":"@site/docs/api/client.md","sourceDirName":"api","slug":"/api/client","permalink":"/benchwise/docs/api/client","draft":false,"unlisted":false,"editUrl":"https://github.com/Benchwise/benchwise/tree/main/website/docs/api/client.md","tags":[],"version":"current","sidebarPosition":8,"frontMatter":{"sidebar_position":8},"sidebar":"docs","previous":{"title":"Configuration","permalink":"/benchwise/docs/api/config"},"next":{"title":"Exceptions","permalink":"/benchwise/docs/api/exceptions"}},{"id":"api/config","title":"Configuration","description":"Configuration management for Benchwise.","source":"@site/docs/api/config.md","sourceDirName":"api","slug":"/api/config","permalink":"/benchwise/docs/api/config","draft":false,"unlisted":false,"editUrl":"https://github.com/Benchwise/benchwise/tree/main/website/docs/api/config.md","tags":[],"version":"current","sidebarPosition":7,"frontMatter":{"sidebar_position":7},"sidebar":"docs","previous":{"title":"Results Analyzer","permalink":"/benchwise/docs/api/results/results-analyzer"},"next":{"title":"Benchwise Client","permalink":"/benchwise/docs/api/client"}},{"id":"api/datasets/create-dataset","title":"Dataset Creation Helpers","description":"Helper functions for creating common dataset types.","source":"@site/docs/api/datasets/create-dataset.md","sourceDirName":"api/datasets","slug":"/api/datasets/create-dataset","permalink":"/benchwise/docs/api/datasets/create-dataset","draft":false,"unlisted":false,"editUrl":"https://github.com/Benchwise/benchwise/tree/main/website/docs/api/datasets/create-dataset.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"docs","previous":{"title":"Load Dataset","permalink":"/benchwise/docs/api/datasets/load-dataset"},"next":{"title":"ModelAdapter","permalink":"/benchwise/docs/api/models/model-adapter"}},{"id":"api/datasets/dataset","title":"Dataset","description":"Main dataset class for organizing evaluation data.","source":"@site/docs/api/datasets/dataset.md","sourceDirName":"api/datasets","slug":"/api/datasets/dataset","permalink":"/benchwise/docs/api/datasets/dataset","draft":false,"unlisted":false,"editUrl":"https://github.com/Benchwise/benchwise/tree/main/website/docs/api/datasets/dataset.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"docs","previous":{"title":"Coherence Score","permalink":"/benchwise/docs/api/metrics/coherence"},"next":{"title":"Load Dataset","permalink":"/benchwise/docs/api/datasets/load-dataset"}},{"id":"api/datasets/load-dataset","title":"Load Dataset","description":"Load datasets from JSON, CSV, or URLs.","source":"@site/docs/api/datasets/load-dataset.md","sourceDirName":"api/datasets","slug":"/api/datasets/load-dataset","permalink":"/benchwise/docs/api/datasets/load-dataset","draft":false,"unlisted":false,"editUrl":"https://github.com/Benchwise/benchwise/tree/main/website/docs/api/datasets/load-dataset.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"docs","previous":{"title":"Dataset","permalink":"/benchwise/docs/api/datasets/dataset"},"next":{"title":"Dataset Creation Helpers","permalink":"/benchwise/docs/api/datasets/create-dataset"}},{"id":"api/decorators/benchmark","title":"Benchmark","description":"Decorator to mark evaluation functions as named benchmarks with metadata.","source":"@site/docs/api/decorators/benchmark.md","sourceDirName":"api/decorators","slug":"/api/decorators/benchmark","permalink":"/benchwise/docs/api/decorators/benchmark","draft":false,"unlisted":false,"editUrl":"https://github.com/Benchwise/benchwise/tree/main/website/docs/api/decorators/benchmark.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"docs","previous":{"title":"Evaluate","permalink":"/benchwise/docs/api/decorators/evaluate"},"next":{"title":"Stress Test","permalink":"/benchwise/docs/api/decorators/stress-test"}},{"id":"api/decorators/evaluate","title":"Evaluate","description":"The main decorator for running evaluations across multiple models.","source":"@site/docs/api/decorators/evaluate.md","sourceDirName":"api/decorators","slug":"/api/decorators/evaluate","permalink":"/benchwise/docs/api/decorators/evaluate","draft":false,"unlisted":false,"editUrl":"https://github.com/Benchwise/benchwise/tree/main/website/docs/api/decorators/evaluate.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"docs","previous":{"title":"Results","permalink":"/benchwise/docs/guides/results"},"next":{"title":"Benchmark","permalink":"/benchwise/docs/api/decorators/benchmark"}},{"id":"api/decorators/stress-test","title":"Stress Test","description":"Decorator for performance and load testing with concurrent requests.","source":"@site/docs/api/decorators/stress-test.md","sourceDirName":"api/decorators","slug":"/api/decorators/stress-test","permalink":"/benchwise/docs/api/decorators/stress-test","draft":false,"unlisted":false,"editUrl":"https://github.com/Benchwise/benchwise/tree/main/website/docs/api/decorators/stress-test.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"docs","previous":{"title":"Benchmark","permalink":"/benchwise/docs/api/decorators/benchmark"},"next":{"title":"Metrics Overview","permalink":"/benchwise/docs/api/metrics/overview"}},{"id":"api/exceptions","title":"Exceptions","description":"Custom exception classes for Benchwise.","source":"@site/docs/api/exceptions.md","sourceDirName":"api","slug":"/api/exceptions","permalink":"/benchwise/docs/api/exceptions","draft":false,"unlisted":false,"editUrl":"https://github.com/Benchwise/benchwise/tree/main/website/docs/api/exceptions.md","tags":[],"version":"current","sidebarPosition":9,"frontMatter":{"sidebar_position":9},"sidebar":"docs","previous":{"title":"Benchwise Client","permalink":"/benchwise/docs/api/client"},"next":{"title":"Question Answering","permalink":"/benchwise/docs/examples/question-answering"}},{"id":"api/metrics/accuracy","title":"Accuracy","description":"Calculate exact match accuracy between predictions and references.","source":"@site/docs/api/metrics/accuracy.md","sourceDirName":"api/metrics","slug":"/api/metrics/accuracy","permalink":"/benchwise/docs/api/metrics/accuracy","draft":false,"unlisted":false,"editUrl":"https://github.com/Benchwise/benchwise/tree/main/website/docs/api/metrics/accuracy.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"docs","previous":{"title":"Metrics Overview","permalink":"/benchwise/docs/api/metrics/overview"},"next":{"title":"ROUGE-L","permalink":"/benchwise/docs/api/metrics/rouge"}},{"id":"api/metrics/bert-score","title":"BERT Score","description":"Calculate BERT-based semantic similarity.","source":"@site/docs/api/metrics/bert-score.md","sourceDirName":"api/metrics","slug":"/api/metrics/bert-score","permalink":"/benchwise/docs/api/metrics/bert-score","draft":false,"unlisted":false,"editUrl":"https://github.com/Benchwise/benchwise/tree/main/website/docs/api/metrics/bert-score.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5},"sidebar":"docs","previous":{"title":"BLEU Score","permalink":"/benchwise/docs/api/metrics/bleu"},"next":{"title":"Semantic Similarity","permalink":"/benchwise/docs/api/metrics/semantic-similarity"}},{"id":"api/metrics/bleu","title":"BLEU Score","description":"Calculate BLEU score for translation and text generation quality.","source":"@site/docs/api/metrics/bleu.md","sourceDirName":"api/metrics","slug":"/api/metrics/bleu","permalink":"/benchwise/docs/api/metrics/bleu","draft":false,"unlisted":false,"editUrl":"https://github.com/Benchwise/benchwise/tree/main/website/docs/api/metrics/bleu.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"docs","previous":{"title":"ROUGE-L","permalink":"/benchwise/docs/api/metrics/rouge"},"next":{"title":"BERT Score","permalink":"/benchwise/docs/api/metrics/bert-score"}},{"id":"api/metrics/coherence","title":"Coherence Score","description":"Evaluate text coherence and quality.","source":"@site/docs/api/metrics/coherence.md","sourceDirName":"api/metrics","slug":"/api/metrics/coherence","permalink":"/benchwise/docs/api/metrics/coherence","draft":false,"unlisted":false,"editUrl":"https://github.com/Benchwise/benchwise/tree/main/website/docs/api/metrics/coherence.md","tags":[],"version":"current","sidebarPosition":8,"frontMatter":{"sidebar_position":8},"sidebar":"docs","previous":{"title":"Safety Score","permalink":"/benchwise/docs/api/metrics/safety"},"next":{"title":"Dataset","permalink":"/benchwise/docs/api/datasets/dataset"}},{"id":"api/metrics/overview","title":"Metrics Overview","description":"Benchwise provides a comprehensive set of evaluation metrics for assessing LLM outputs.","source":"@site/docs/api/metrics/overview.md","sourceDirName":"api/metrics","slug":"/api/metrics/overview","permalink":"/benchwise/docs/api/metrics/overview","draft":false,"unlisted":false,"editUrl":"https://github.com/Benchwise/benchwise/tree/main/website/docs/api/metrics/overview.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"docs","previous":{"title":"Stress Test","permalink":"/benchwise/docs/api/decorators/stress-test"},"next":{"title":"Accuracy","permalink":"/benchwise/docs/api/metrics/accuracy"}},{"id":"api/metrics/rouge","title":"ROUGE-L","description":"Calculate ROUGE-L score for text overlap evaluation.","source":"@site/docs/api/metrics/rouge.md","sourceDirName":"api/metrics","slug":"/api/metrics/rouge","permalink":"/benchwise/docs/api/metrics/rouge","draft":false,"unlisted":false,"editUrl":"https://github.com/Benchwise/benchwise/tree/main/website/docs/api/metrics/rouge.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"docs","previous":{"title":"Accuracy","permalink":"/benchwise/docs/api/metrics/accuracy"},"next":{"title":"BLEU Score","permalink":"/benchwise/docs/api/metrics/bleu"}},{"id":"api/metrics/safety","title":"Safety Score","description":"Evaluate content safety of generated texts.","source":"@site/docs/api/metrics/safety.md","sourceDirName":"api/metrics","slug":"/api/metrics/safety","permalink":"/benchwise/docs/api/metrics/safety","draft":false,"unlisted":false,"editUrl":"https://github.com/Benchwise/benchwise/tree/main/website/docs/api/metrics/safety.md","tags":[],"version":"current","sidebarPosition":7,"frontMatter":{"sidebar_position":7},"sidebar":"docs","previous":{"title":"Semantic Similarity","permalink":"/benchwise/docs/api/metrics/semantic-similarity"},"next":{"title":"Coherence Score","permalink":"/benchwise/docs/api/metrics/coherence"}},{"id":"api/metrics/semantic-similarity","title":"Semantic Similarity","description":"Calculate embedding-based semantic similarity.","source":"@site/docs/api/metrics/semantic-similarity.md","sourceDirName":"api/metrics","slug":"/api/metrics/semantic-similarity","permalink":"/benchwise/docs/api/metrics/semantic-similarity","draft":false,"unlisted":false,"editUrl":"https://github.com/Benchwise/benchwise/tree/main/website/docs/api/metrics/semantic-similarity.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"sidebar_position":6},"sidebar":"docs","previous":{"title":"BERT Score","permalink":"/benchwise/docs/api/metrics/bert-score"},"next":{"title":"Safety Score","permalink":"/benchwise/docs/api/metrics/safety"}},{"id":"api/models/anthropic","title":"AnthropicAdapter","description":"Adapter for Anthropic Claude models.","source":"@site/docs/api/models/anthropic.md","sourceDirName":"api/models","slug":"/api/models/anthropic","permalink":"/benchwise/docs/api/models/anthropic","draft":false,"unlisted":false,"editUrl":"https://github.com/Benchwise/benchwise/tree/main/website/docs/api/models/anthropic.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"docs","previous":{"title":"OpenAIAdapter","permalink":"/benchwise/docs/api/models/openai"},"next":{"title":"GoogleAdapter","permalink":"/benchwise/docs/api/models/google"}},{"id":"api/models/google","title":"GoogleAdapter","description":"Adapter for Google Gemini models.","source":"@site/docs/api/models/google.md","sourceDirName":"api/models","slug":"/api/models/google","permalink":"/benchwise/docs/api/models/google","draft":false,"unlisted":false,"editUrl":"https://github.com/Benchwise/benchwise/tree/main/website/docs/api/models/google.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"docs","previous":{"title":"AnthropicAdapter","permalink":"/benchwise/docs/api/models/anthropic"},"next":{"title":"HuggingFaceAdapter","permalink":"/benchwise/docs/api/models/huggingface"}},{"id":"api/models/huggingface","title":"HuggingFaceAdapter","description":"Adapter for HuggingFace models.","source":"@site/docs/api/models/huggingface.md","sourceDirName":"api/models","slug":"/api/models/huggingface","permalink":"/benchwise/docs/api/models/huggingface","draft":false,"unlisted":false,"editUrl":"https://github.com/Benchwise/benchwise/tree/main/website/docs/api/models/huggingface.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5},"sidebar":"docs","previous":{"title":"GoogleAdapter","permalink":"/benchwise/docs/api/models/google"},"next":{"title":"Evaluation Result","permalink":"/benchwise/docs/api/results/evaluation-result"}},{"id":"api/models/model-adapter","title":"ModelAdapter","description":"Abstract base class for all model adapters.","source":"@site/docs/api/models/model-adapter.md","sourceDirName":"api/models","slug":"/api/models/model-adapter","permalink":"/benchwise/docs/api/models/model-adapter","draft":false,"unlisted":false,"editUrl":"https://github.com/Benchwise/benchwise/tree/main/website/docs/api/models/model-adapter.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"docs","previous":{"title":"Dataset Creation Helpers","permalink":"/benchwise/docs/api/datasets/create-dataset"},"next":{"title":"OpenAIAdapter","permalink":"/benchwise/docs/api/models/openai"}},{"id":"api/models/openai","title":"OpenAIAdapter","description":"Adapter for OpenAI models (GPT-3.5, GPT-4, etc.).","source":"@site/docs/api/models/openai.md","sourceDirName":"api/models","slug":"/api/models/openai","permalink":"/benchwise/docs/api/models/openai","draft":false,"unlisted":false,"editUrl":"https://github.com/Benchwise/benchwise/tree/main/website/docs/api/models/openai.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"docs","previous":{"title":"ModelAdapter","permalink":"/benchwise/docs/api/models/model-adapter"},"next":{"title":"AnthropicAdapter","permalink":"/benchwise/docs/api/models/anthropic"}},{"id":"api/overview","title":"API Reference Overview","description":"Benchwise provides a comprehensive API for LLM evaluation. This section documents all public APIs.","source":"@site/docs/api/overview.md","sourceDirName":"api","slug":"/api/overview","permalink":"/benchwise/docs/api/overview","draft":false,"unlisted":false,"editUrl":"https://github.com/Benchwise/benchwise/tree/main/website/docs/api/overview.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1}},{"id":"api/results/benchmark-result","title":"Benchmark Result","description":"Container for organizing multiple evaluation results.","source":"@site/docs/api/results/benchmark-result.md","sourceDirName":"api/results","slug":"/api/results/benchmark-result","permalink":"/benchwise/docs/api/results/benchmark-result","draft":false,"unlisted":false,"editUrl":"https://github.com/Benchwise/benchwise/tree/main/website/docs/api/results/benchmark-result.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"docs","previous":{"title":"Evaluation Result","permalink":"/benchwise/docs/api/results/evaluation-result"},"next":{"title":"Results Analyzer","permalink":"/benchwise/docs/api/results/results-analyzer"}},{"id":"api/results/evaluation-result","title":"Evaluation Result","description":"Result from a single model evaluation.","source":"@site/docs/api/results/evaluation-result.md","sourceDirName":"api/results","slug":"/api/results/evaluation-result","permalink":"/benchwise/docs/api/results/evaluation-result","draft":false,"unlisted":false,"editUrl":"https://github.com/Benchwise/benchwise/tree/main/website/docs/api/results/evaluation-result.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"docs","previous":{"title":"HuggingFaceAdapter","permalink":"/benchwise/docs/api/models/huggingface"},"next":{"title":"Benchmark Result","permalink":"/benchwise/docs/api/results/benchmark-result"}},{"id":"api/results/results-analyzer","title":"Results Analyzer","description":"Statistical analysis and reporting for results.","source":"@site/docs/api/results/results-analyzer.md","sourceDirName":"api/results","slug":"/api/results/results-analyzer","permalink":"/benchwise/docs/api/results/results-analyzer","draft":false,"unlisted":false,"editUrl":"https://github.com/Benchwise/benchwise/tree/main/website/docs/api/results/results-analyzer.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"docs","previous":{"title":"Benchmark Result","permalink":"/benchwise/docs/api/results/benchmark-result"},"next":{"title":"Configuration","permalink":"/benchwise/docs/api/config"}},{"id":"changelog","title":"Changelog","description":"Track changes and updates to Benchwise.","source":"@site/docs/changelog.md","sourceDirName":".","slug":"/changelog","permalink":"/benchwise/docs/changelog","draft":false,"unlisted":false,"editUrl":"https://github.com/Benchwise/benchwise/tree/main/website/docs/changelog.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"docs","previous":{"title":"CLI","permalink":"/benchwise/docs/cli"},"next":{"title":"Migration Guide","permalink":"/benchwise/docs/migration-guide"}},{"id":"cli","title":"CLI","description":"Command-line interface for Benchwise.","source":"@site/docs/cli.md","sourceDirName":".","slug":"/cli","permalink":"/benchwise/docs/cli","draft":false,"unlisted":false,"editUrl":"https://github.com/Benchwise/benchwise/tree/main/website/docs/cli.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"docs","previous":{"title":"Offline Mode","permalink":"/benchwise/docs/advanced/offline-mode"},"next":{"title":"Changelog","permalink":"/benchwise/docs/changelog"}},{"id":"contributing","title":"Contributing","description":"Contribute to Benchwise development.","source":"@site/docs/contributing.md","sourceDirName":".","slug":"/contributing","permalink":"/benchwise/docs/contributing","draft":false,"unlisted":false,"editUrl":"https://github.com/Benchwise/benchwise/tree/main/website/docs/contributing.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"docs","previous":{"title":"Migration Guide","permalink":"/benchwise/docs/migration-guide"},"next":{"title":"FAQ","permalink":"/benchwise/docs/faq"}},{"id":"examples","title":"Examples","description":"Real-world examples of using Benchwise for LLM evaluation.","source":"@site/docs/examples.md","sourceDirName":".","slug":"/examples","permalink":"/benchwise/docs/examples","draft":false,"unlisted":false,"editUrl":"https://github.com/Benchwise/benchwise/tree/main/website/docs/examples.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3}},{"id":"examples/classification","title":"Classification","description":"Evaluate models on text classification tasks.","source":"@site/docs/examples/classification.md","sourceDirName":"examples","slug":"/examples/classification","permalink":"/benchwise/docs/examples/classification","draft":false,"unlisted":false,"editUrl":"https://github.com/Benchwise/benchwise/tree/main/website/docs/examples/classification.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"docs","previous":{"title":"Safety Evaluation","permalink":"/benchwise/docs/examples/safety-evaluation"},"next":{"title":"Multi-Model Comparison","permalink":"/benchwise/docs/examples/multi-model-comparison"}},{"id":"examples/multi-model-comparison","title":"Multi-Model Comparison","description":"Compare multiple models across different tasks and metrics.","source":"@site/docs/examples/multi-model-comparison.md","sourceDirName":"examples","slug":"/examples/multi-model-comparison","permalink":"/benchwise/docs/examples/multi-model-comparison","draft":false,"unlisted":false,"editUrl":"https://github.com/Benchwise/benchwise/tree/main/website/docs/examples/multi-model-comparison.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5},"sidebar":"docs","previous":{"title":"Classification","permalink":"/benchwise/docs/examples/classification"},"next":{"title":"Configuration","permalink":"/benchwise/docs/advanced/configuration"}},{"id":"examples/question-answering","title":"Question Answering","description":"Evaluate models on question answering tasks.","source":"@site/docs/examples/question-answering.md","sourceDirName":"examples","slug":"/examples/question-answering","permalink":"/benchwise/docs/examples/question-answering","draft":false,"unlisted":false,"editUrl":"https://github.com/Benchwise/benchwise/tree/main/website/docs/examples/question-answering.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"docs","previous":{"title":"Exceptions","permalink":"/benchwise/docs/api/exceptions"},"next":{"title":"Summarization","permalink":"/benchwise/docs/examples/summarization"}},{"id":"examples/safety-evaluation","title":"Safety Evaluation","description":"Evaluate model safety and content filtering capabilities.","source":"@site/docs/examples/safety-evaluation.md","sourceDirName":"examples","slug":"/examples/safety-evaluation","permalink":"/benchwise/docs/examples/safety-evaluation","draft":false,"unlisted":false,"editUrl":"https://github.com/Benchwise/benchwise/tree/main/website/docs/examples/safety-evaluation.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"docs","previous":{"title":"Summarization","permalink":"/benchwise/docs/examples/summarization"},"next":{"title":"Classification","permalink":"/benchwise/docs/examples/classification"}},{"id":"examples/summarization","title":"Summarization","description":"Evaluate text summarization quality using ROUGE and other metrics.","source":"@site/docs/examples/summarization.md","sourceDirName":"examples","slug":"/examples/summarization","permalink":"/benchwise/docs/examples/summarization","draft":false,"unlisted":false,"editUrl":"https://github.com/Benchwise/benchwise/tree/main/website/docs/examples/summarization.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"docs","previous":{"title":"Question Answering","permalink":"/benchwise/docs/examples/question-answering"},"next":{"title":"Safety Evaluation","permalink":"/benchwise/docs/examples/safety-evaluation"}},{"id":"faq","title":"FAQ","description":"Frequently asked questions about Benchwise.","source":"@site/docs/faq.md","sourceDirName":".","slug":"/faq","permalink":"/benchwise/docs/faq","draft":false,"unlisted":false,"editUrl":"https://github.com/Benchwise/benchwise/tree/main/website/docs/faq.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5},"sidebar":"docs","previous":{"title":"Contributing","permalink":"/benchwise/docs/contributing"}},{"id":"getting-started","title":"Getting Started with Benchwise","description":"Welcome to Benchwise! This guide will help you get up and running with evaluating LLMs using our platform.","source":"@site/docs/getting-started.md","sourceDirName":".","slug":"/getting-started","permalink":"/benchwise/docs/getting-started","draft":false,"unlisted":false,"editUrl":"https://github.com/Benchwise/benchwise/tree/main/website/docs/getting-started.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2}},{"id":"getting-started/core-concepts","title":"Core Concepts","description":"Understand the key concepts that power Benchwise.","source":"@site/docs/getting-started/core-concepts.md","sourceDirName":"getting-started","slug":"/getting-started/core-concepts","permalink":"/benchwise/docs/getting-started/core-concepts","draft":false,"unlisted":false,"editUrl":"https://github.com/Benchwise/benchwise/tree/main/website/docs/getting-started/core-concepts.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"docs","previous":{"title":"Quickstart","permalink":"/benchwise/docs/getting-started/quickstart"},"next":{"title":"Evaluation","permalink":"/benchwise/docs/guides/evaluation"}},{"id":"getting-started/installation","title":"Installation","description":"Get started with Benchwise by installing the SDK and setting up your environment.","source":"@site/docs/getting-started/installation.md","sourceDirName":"getting-started","slug":"/getting-started/installation","permalink":"/benchwise/docs/getting-started/installation","draft":false,"unlisted":false,"editUrl":"https://github.com/Benchwise/benchwise/tree/main/website/docs/getting-started/installation.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"docs","previous":{"title":"Welcome to Benchwise","permalink":"/benchwise/docs/"},"next":{"title":"Quickstart","permalink":"/benchwise/docs/getting-started/quickstart"}},{"id":"getting-started/quickstart","title":"Quickstart","description":"Run your first LLM evaluation in under 5 minutes.","source":"@site/docs/getting-started/quickstart.md","sourceDirName":"getting-started","slug":"/getting-started/quickstart","permalink":"/benchwise/docs/getting-started/quickstart","draft":false,"unlisted":false,"editUrl":"https://github.com/Benchwise/benchwise/tree/main/website/docs/getting-started/quickstart.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"docs","previous":{"title":"Installation","permalink":"/benchwise/docs/getting-started/installation"},"next":{"title":"Core Concepts","permalink":"/benchwise/docs/getting-started/core-concepts"}},{"id":"guides/datasets","title":"Datasets","description":"Master dataset creation and management in Benchwise.","source":"@site/docs/guides/datasets.md","sourceDirName":"guides","slug":"/guides/datasets","permalink":"/benchwise/docs/guides/datasets","draft":false,"unlisted":false,"editUrl":"https://github.com/Benchwise/benchwise/tree/main/website/docs/guides/datasets.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"docs","previous":{"title":"Metrics","permalink":"/benchwise/docs/guides/metrics"},"next":{"title":"Models","permalink":"/benchwise/docs/guides/models"}},{"id":"guides/evaluation","title":"Evaluation","description":"Learn how to create effective LLM evaluations with Benchwise.","source":"@site/docs/guides/evaluation.md","sourceDirName":"guides","slug":"/guides/evaluation","permalink":"/benchwise/docs/guides/evaluation","draft":false,"unlisted":false,"editUrl":"https://github.com/Benchwise/benchwise/tree/main/website/docs/guides/evaluation.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"docs","previous":{"title":"Core Concepts","permalink":"/benchwise/docs/getting-started/core-concepts"},"next":{"title":"Metrics","permalink":"/benchwise/docs/guides/metrics"}},{"id":"guides/metrics","title":"Metrics","description":"Learn how to use and combine evaluation metrics in Benchwise.","source":"@site/docs/guides/metrics.md","sourceDirName":"guides","slug":"/guides/metrics","permalink":"/benchwise/docs/guides/metrics","draft":false,"unlisted":false,"editUrl":"https://github.com/Benchwise/benchwise/tree/main/website/docs/guides/metrics.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"docs","previous":{"title":"Evaluation","permalink":"/benchwise/docs/guides/evaluation"},"next":{"title":"Datasets","permalink":"/benchwise/docs/guides/datasets"}},{"id":"guides/models","title":"Models","description":"Understand how to work with different LLM providers in Benchwise.","source":"@site/docs/guides/models.md","sourceDirName":"guides","slug":"/guides/models","permalink":"/benchwise/docs/guides/models","draft":false,"unlisted":false,"editUrl":"https://github.com/Benchwise/benchwise/tree/main/website/docs/guides/models.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"docs","previous":{"title":"Datasets","permalink":"/benchwise/docs/guides/datasets"},"next":{"title":"Results","permalink":"/benchwise/docs/guides/results"}},{"id":"guides/results","title":"Results","description":"Learn how to manage, analyze, and export evaluation results.","source":"@site/docs/guides/results.md","sourceDirName":"guides","slug":"/guides/results","permalink":"/benchwise/docs/guides/results","draft":false,"unlisted":false,"editUrl":"https://github.com/Benchwise/benchwise/tree/main/website/docs/guides/results.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5},"sidebar":"docs","previous":{"title":"Models","permalink":"/benchwise/docs/guides/models"},"next":{"title":"Evaluate","permalink":"/benchwise/docs/api/decorators/evaluate"}},{"id":"index","title":"Welcome to Benchwise","description":"Benchwise is an open-source Python SDK for LLM evaluation with PyTest-like syntax. It allows you to create custom evaluations, run benchmarks across multiple models, and share results with the community.","source":"@site/docs/index.md","sourceDirName":".","slug":"/","permalink":"/benchwise/docs/","draft":false,"unlisted":false,"editUrl":"https://github.com/Benchwise/benchwise/tree/main/website/docs/index.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"slug":"/","title":"Welcome to Benchwise"},"sidebar":"docs","next":{"title":"Installation","permalink":"/benchwise/docs/getting-started/installation"}},{"id":"intro","title":"Welcome to Benchwise","description":"Benchwise is an open-source Python SDK for LLM evaluation with PyTest-like syntax. It allows you to create custom evaluations, run benchmarks across multiple models, and share results with the community.","source":"@site/docs/intro.md","sourceDirName":".","slug":"/","permalink":"/benchwise/docs/","draft":false,"unlisted":false,"editUrl":"https://github.com/Benchwise/benchwise/tree/main/website/docs/intro.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"slug":"/"}},{"id":"migration-guide","title":"Migration Guide","description":"Guide for migrating between Benchwise versions.","source":"@site/docs/migration-guide.md","sourceDirName":".","slug":"/migration-guide","permalink":"/benchwise/docs/migration-guide","draft":false,"unlisted":false,"editUrl":"https://github.com/Benchwise/benchwise/tree/main/website/docs/migration-guide.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"docs","previous":{"title":"Changelog","permalink":"/benchwise/docs/changelog"},"next":{"title":"Contributing","permalink":"/benchwise/docs/contributing"}},{"id":"usage-guide","title":"Benchwise SDK: Complete Usage Guide","description":"Welcome to Benchwise - the GitHub of LLM evaluation! This guide will walk you through everything you need to know to get started with evaluating your language models.","source":"@site/docs/usage-guide.md","sourceDirName":".","slug":"/usage-guide","permalink":"/benchwise/docs/usage-guide","draft":false,"unlisted":false,"editUrl":"https://github.com/Benchwise/benchwise/tree/main/website/docs/usage-guide.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3}}],"drafts":[],"sidebars":{"docs":[{"type":"doc","id":"index"},{"type":"category","label":"Getting Started","collapsed":false,"items":[{"type":"doc","id":"getting-started/installation"},{"type":"doc","id":"getting-started/quickstart"},{"type":"doc","id":"getting-started/core-concepts"}],"collapsible":true},{"type":"category","label":"Guides","items":[{"type":"doc","id":"guides/evaluation"},{"type":"doc","id":"guides/metrics"},{"type":"doc","id":"guides/datasets"},{"type":"doc","id":"guides/models"},{"type":"doc","id":"guides/results"}],"collapsed":true,"collapsible":true},{"type":"category","label":"API Reference","items":[{"type":"category","label":"Decorators","items":[{"type":"doc","id":"api/decorators/evaluate"},{"type":"doc","id":"api/decorators/benchmark"},{"type":"doc","id":"api/decorators/stress-test"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Metrics","items":[{"type":"doc","id":"api/metrics/overview"},{"type":"doc","id":"api/metrics/accuracy"},{"type":"doc","id":"api/metrics/rouge"},{"type":"doc","id":"api/metrics/bleu"},{"type":"doc","id":"api/metrics/bert-score"},{"type":"doc","id":"api/metrics/semantic-similarity"},{"type":"doc","id":"api/metrics/safety"},{"type":"doc","id":"api/metrics/coherence"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Datasets","items":[{"type":"doc","id":"api/datasets/dataset"},{"type":"doc","id":"api/datasets/load-dataset"},{"type":"doc","id":"api/datasets/create-dataset"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Models","items":[{"type":"doc","id":"api/models/model-adapter"},{"type":"doc","id":"api/models/openai"},{"type":"doc","id":"api/models/anthropic"},{"type":"doc","id":"api/models/google"},{"type":"doc","id":"api/models/huggingface"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Results","items":[{"type":"doc","id":"api/results/evaluation-result"},{"type":"doc","id":"api/results/benchmark-result"},{"type":"doc","id":"api/results/results-analyzer"}],"collapsed":true,"collapsible":true},{"type":"doc","id":"api/config"},{"type":"doc","id":"api/client"},{"type":"doc","id":"api/exceptions"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Examples","items":[{"type":"doc","id":"examples/question-answering"},{"type":"doc","id":"examples/summarization"},{"type":"doc","id":"examples/safety-evaluation"},{"type":"doc","id":"examples/classification"},{"type":"doc","id":"examples/multi-model-comparison"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Advanced","items":[{"type":"doc","id":"advanced/configuration"},{"type":"doc","id":"advanced/api-integration"},{"type":"doc","id":"advanced/custom-metrics"},{"type":"doc","id":"advanced/error-handling"},{"type":"doc","id":"advanced/logging"},{"type":"doc","id":"advanced/offline-mode"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Additional","items":[{"type":"doc","id":"cli"},{"type":"doc","id":"changelog"},{"type":"doc","id":"migration-guide"},{"type":"doc","id":"contributing"},{"type":"doc","id":"faq"}],"collapsed":true,"collapsible":true}]}}]}},"docusaurus-plugin-content-blog":{"default":{"blogSidebarTitle":"Recent posts","blogPosts":[],"blogListPaginated":[],"blogTags":{},"blogTagsListPath":"/benchwise/blog/tags"}},"docusaurus-plugin-content-pages":{"default":[{"type":"jsx","permalink":"/benchwise/","source":"@site/src/pages/index.tsx"},{"type":"mdx","permalink":"/benchwise/markdown-page","source":"@site/src/pages/markdown-page.md","title":"Markdown page example","description":"You don't need React to write simple standalone pages.","frontMatter":{"title":"Markdown page example"},"unlisted":false}]},"docusaurus-plugin-debug":{},"docusaurus-plugin-svgr":{},"docusaurus-theme-classic":{},"docusaurus-bootstrap-plugin":{},"docusaurus-mdx-fallback-plugin":{}}}